{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will explain how to integrate deep learning models into your Brancher pipline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building neural network models\n",
    "\n",
    "All ```pytorch``` functions can be used in Brancher. These functions need to be imported from the Brancher `function` module which contains all ```pytorch``` functions acting on ```torch.Tensor``` in the form of Brancher functions acting on ```brancher.Variable```.\n",
    "\n",
    "We will define a stochastic convolutional network on the MNIST dataset. The first step is to import the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "# Create the data\n",
    "image_size  = 28\n",
    "num_classes = 10\n",
    "\n",
    "train = torchvision.datasets.MNIST(root='./data',   train=True,  download=True, transform=None)\n",
    "test  = torchvision.datasets.MNIST(root='./dataSo', train=False, download=True, transform=None)\n",
    "\n",
    "dataset_size   = len(train)\n",
    "input_variable = np.reshape(train.train_data.numpy(), newshape=(dataset_size, 1, image_size, image_size))\n",
    "output_labels  = train.train_labels.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Brancher model, datasets are stored in empirical variables. These are random variables that sample minibatches from a dataset.\n",
    "\n",
    "In a supervised problem we need two empirical variables, one for the input images and the other for the labels. However, these two variables need to be sampled jointly as each image should be associated with its correct label. In Brancher, we can implement this by generating a random variable ```RandomIndices```, and use it as ```indices``` for images and labels: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brancher.standard_variables import EmpiricalVariable as Empirical\n",
    "from brancher.standard_variables import RandomIndices\n",
    "\n",
    "# Data sampling model\n",
    "minibatch_size = 7\n",
    "\n",
    "minibatch_indices = RandomIndices( dataset_size=dataset_size, batch_size=minibatch_size, \n",
    "                                   name=\"indices\", is_observed=True )\n",
    "\n",
    "x = Empirical( input_variable, indices=minibatch_indices, \n",
    "               name=\"x\", is_observed=True )\n",
    "\n",
    "labels = Empirical( output_labels, indices=minibatch_indices, \n",
    "                    name=\"labels\", is_observed=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step we import the `pytorch` function `conv2d` from `brancher.functions` and use it to build a stochastic 2D-convolutional layer with Gaussian weights. \n",
    "\n",
    "In order to do this we define the weights as a `NormalVariable`. We then use the Brancher `conv2d` and `relu` functions on the weights `Wk` and images `x`, and wrap the result of the layer `z` inside a `DeterministicVariable`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brancher import functions as BF\n",
    "\n",
    "from brancher.standard_variables import DeterministicVariable as Deterministic\n",
    "from brancher.standard_variables import NormalVariable as Normal\n",
    "from brancher.standard_variables import CategoricalVariable as Categorical\n",
    "\n",
    "in_channels  = 1\n",
    "out_channels = 5\n",
    "image_size   = 28\n",
    "\n",
    "# Define Gaussian convolutional kernels:\n",
    "Wk = Normal( loc=np.zeros((out_channels, in_channels, 3, 3)),\n",
    "             scale=np.ones((out_channels, in_channels, 3, 3)),\n",
    "             name=\"Wk\")\n",
    "\n",
    "# Define output: \n",
    "z = Deterministic( BF.relu(BF.conv2d(x, Wk, padding=1)), name=\"z\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The randomized input image `x` is convolved with the random convolutional filter weights `Wk`. We can now run the forward pass by sampling from the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 6\n",
    "z.get_sample(num_samples)[\"z\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in each of these samples both the input and the weigths are sampled independently.\n",
    "\n",
    "We can now add a linear layer to this result to get a shallow convolutional classifier. We can do this by again defining random parameter variables as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "Wl = Normal( loc   = np.zeros((num_classes, image_size*image_size*out_channels)),\n",
    "             scale = np.ones((num_classes, image_size*image_size*out_channels)),\n",
    "             name  = \"Wl\")\n",
    "\n",
    "b  = Normal( loc   = np.zeros((num_classes, 1)),\n",
    "             scale = np.ones((num_classes, 1)),\n",
    "             name  = \"b\")\n",
    "\n",
    "reshaped_z = BF.reshape(z, shape=(image_size*image_size*out_channels, 1))\n",
    "\n",
    "k  = Categorical( logits = BF.linear(reshaped_z, Wl, b), \n",
    "                  name=\"k\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had to reshape (flatten) the variable `z` to use it as input to the linear layer. Note that in Brancher you never need to explicitly consider the batch dimension. Batch properties are part of the data, not of the model and Brancher will handle them automatically!\n",
    "\n",
    "### Observing the model and training the weights ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is defined we need to specifiy which variables are observed. The input image variable `x` was set up to be observed during model definition. The other variable to observe is the output `k` which needs to be observed using the real labels. To this aim, we simply need to call the `.observe` method on `k` with the label `EmpiricalVariable` as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k.observe(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done! We are now ready to learn the weights. If we are not concerned with quantifying uncertainty, we can train using maximum-a-posteriori (MAP). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brancher.inference import MAP\n",
    "from brancher.inference import perform_inference\n",
    "from brancher.variables import ProbabilisticModel\n",
    "\n",
    "convolutional_model = ProbabilisticModel([k])\n",
    "\n",
    "perform_inference( convolutional_model,\n",
    "                   inference_method=MAP(),\n",
    "                   number_iterations=600, #500\n",
    "                   optimizer=\"Adam\",\n",
    "                   lr=0.005 )\n",
    "loss_list = convolutional_model.diagnostics[\"loss curve\"]\n",
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the model on the test set by sampling the newly trained posterior distribution. To this aim we need to call the `.get_posterior_samples` method since `.get_sample` will result on a sample from the (untrained) prior. We also need to provide the test images as input to the this method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = test.test_data.numpy().shape[0]\n",
    "test_images = np.reshape(test.test_data.numpy(), newshape=(test_size, 1, image_size, image_size))\n",
    "\n",
    "posterior_samples = convolutional_model.get_posterior_sample(num_samples = 1, \n",
    "                                                             input_values = {x: test_images[0:4,:]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is one-hot encoded. Let's get the predicted labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(posterior_samples[\"k\"][0], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using existing PyTorch models in a Brancher model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen how we can construct Brancher models with neural network building blocks. However, there are situations where we want to specify a `pytorch` model and use it within Brancher. In this case we can wrap the `pytorch` model using the `brancher.functions.BrancherFunction` class. \n",
    "\n",
    "Let's start by defining a `pytorch` neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class PytorchNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PytorchNetwork, self).__init__()\n",
    "        out_channels = 5\n",
    "        image_size = 28\n",
    "        self.l1 = torch.nn.Conv2d(in_channels=1, out_channels=out_channels, kernel_size=3, padding=1)\n",
    "        self.f1 = torch.nn.ReLU()\n",
    "        self.l2 = torch.nn.Linear(in_features=image_size ** 2 * out_channels, out_features=10)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = self.f1(self.l1(x))\n",
    "        h_shape = h.shape\n",
    "        h = h.view((h_shape[0], np.prod(h_shape[1:])))\n",
    "        logits = self.l2(h)\n",
    "        return logits\n",
    "    \n",
    "network = PytorchNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert it into a brancher function via `BrancherFunction`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Equivalent Brancher model ##\n",
    "brancher_network = BF.BrancherFunction(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However note that here we **do not** create latent variables. Instead we are treating the network as a black-box **function** with learnable parameters (learned via standard `pytorch`). We can not use Bayesian inference methods on the weights. You can learn this black-box-function via maximal likelihood. \n",
    "\n",
    "We can construct a full Brancher model as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data sampling model #\n",
    "minibatch_size = 4\n",
    "minibatch_indices = RandomIndices( dataset_size=dataset_size, batch_size=minibatch_size,\n",
    "                                   name=\"indices\", is_observed=True )\n",
    "x = Empirical( input_variable, indices=minibatch_indices,\n",
    "               name=\"x\", is_observed=True )\n",
    "labels = Empirical( output_labels, indices=minibatch_indices,\n",
    "                    name=\"labels\", is_observed=True )\n",
    "\n",
    "# Forward model #\n",
    "k = Categorical( logits=brancher_network(x),\n",
    "                 name=\"k\" )\n",
    "k.observe(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train using `MaximumLikelihood`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brancher.inference import MaximumLikelihood\n",
    "from brancher.inference import perform_inference\n",
    "from brancher.variables import ProbabilisticModel\n",
    "\n",
    "convolutional_model = ProbabilisticModel([k])\n",
    "\n",
    "perform_inference( convolutional_model,\n",
    "                   inference_method=MaximumLikelihood(),\n",
    "                   number_iterations=500,\n",
    "                   optimizer=\"Adam\",\n",
    "                   lr=0.001 )\n",
    "loss_list = convolutional_model.diagnostics[\"loss curve\"]\n",
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `MaximumLikelihood` trains the probabilistic model itself (the prior in Bayesian terms) and not the posterior model. Therefore, we can test the model by calling the `.get_sample` method instead√≤f `.get_posterior_sample` (the posterior is not even defined in this model as we do not have latent variables):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = test.test_data.numpy().shape[0]\n",
    "test_images = np.reshape(test.test_data.numpy(), newshape=(test_size, 1, image_size, image_size))\n",
    "\n",
    "np.argmax(convolutional_model.get_sample(1, input_values= {x: test_images[0:4,:]})[\"k\"][0], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
